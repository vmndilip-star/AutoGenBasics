{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d14fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 5\n",
      "python-dotenv could not parse statement starting at line 6\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4050f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "teacher = AssistantAgent(\n",
    "    name=\"Teacher\",\n",
    "    model_client=model_client,\n",
    "    description=\"Explains a topic step by step\",\n",
    "    system_message=\"You are a patient teacher. Explain concepts clearly and step by step.\"\n",
    ")\n",
    "\n",
    "quiz_master = AssistantAgent(\n",
    "    name=\"QuizMaster\",\n",
    "    model_client=model_client,\n",
    "    description=\"Generates practice questions\",\n",
    "    system_message=\"You create 3-5 practice questions based on the Teacher's explanation.\"\n",
    ")\n",
    "\n",
    "grader = AssistantAgent(\n",
    "    name=\"Grader\",\n",
    "    model_client=model_client,\n",
    "    description=\"Checks answers and provides feedback\",\n",
    "    system_message=\"You grade the learner's answers fairly. Keep feedback under 50 words.\"\n",
    ")\n",
    "\n",
    "motivator = AssistantAgent(\n",
    "    name=\"Motivator\",\n",
    "    model_client=model_client,\n",
    "    description=\"Encourages the learner\",\n",
    "    system_message=\"You provide encouragement, tips, and motivation. Be supportive and brief.Say 'TERMINATE' when the learner seems confident.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3ab4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "termination_condition = TextMentionTermination(text=\"TERMINATE\") \n",
    "\n",
    "edu_team = RoundRobinGroupChat(\n",
    "    participants=[teacher, quiz_master, grader, motivator],\n",
    "    termination_condition=termination_condition,\n",
    "    max_turns=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f304df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain about RAG Pipeline.\n",
      "Teacher: Certainly! The RAG (Retrieval-Augmented Generation) pipeline is a framework that combines information retrieval and text generation. It is particularly useful for tasks that require producing or generating informative text-based responses by leveraging large datasets or documents. Let's break it down step-by-step:\n",
      "\n",
      "### 1. Understanding the Components:\n",
      "\n",
      "#### a. Retrieval:\n",
      "- **Purpose:** To find relevant documents or pieces of information from a large dataset or corpus that can potentially aid in generating a more informed and accurate response.\n",
      "- **Method:** Typically done using a retriever model, which could be a traditional Information Retrieval (IR) method (like TF-IDF or BM25) or more advanced methods using neural networks (like Dense Passage Retrieval - DPR).\n",
      "- **Output:** The retrieval step provides a subset of documents or passages deemed highly relevant to the query or input.\n",
      "\n",
      "#### b. Generation:\n",
      "- **Purpose:** To synthesize a coherent and relevant response using the information retrieved.\n",
      "- **Method:** Achieved using a generative model, typically a transformer-based model such as GPT or BART.\n",
      "- **Output:** The generative model creates a response, often by conditioning on the retrieved documents as additional context to improve the accuracy and informativeness of the output.\n",
      "\n",
      "### 2. RAG Pipeline Workflow:\n",
      "\n",
      "1. **Input Query:** The process starts with an input, which can be a question or a prompt requiring a detailed answer.\n",
      "   \n",
      "2. **Retrieval Process:** \n",
      "   - The retrieval model searches through a large corpus to identify documents or passages that are relevant to the query.\n",
      "   - These documents are usually ranked based on relevance scores.\n",
      "\n",
      "3. **Augmentation:** \n",
      "   - The top-ranked documents (e.g., top 5 or top 10) are selected and fed into the generative model along with the original query.\n",
      "   - These documents act as external knowledge sources that enrich the generative model's context.\n",
      "\n",
      "4. **Text Generation:** \n",
      "   - The generative model, using the query and the retrieved documents, generates a response that ideally integrates and utilizes the information from the retrieved documents.\n",
      "   - The output is a coherent text response that answers the query by drawing from both the retrieval results and the generative model's own capabilities.\n",
      "\n",
      "### 3. Benefits of RAG:\n",
      "\n",
      "- **Improved Accuracy and Contextuality:** By augmenting generation with retrieval, the output is generally more accurate as it is based on actual retrieved data rather than relying solely on pre-trained model weights.\n",
      "  \n",
      "- **Handling Long-Tail Queries:** RAG models can effectively manage queries that may not have been covered well in the training data by retrieving up-to-date or domain-specific documents.\n",
      "\n",
      "- **Scalability:** As RAG splits retrieval from generation, it can handle very large datasets efficiently using scalable retrieval systems.\n",
      "\n",
      "### 4. Use Cases:\n",
      "\n",
      "- **Question Answering Systems:** For generating answers to complex or specific queries where real-world data retrieval improves the response quality.\n",
      "  \n",
      "- **Customer Support Automation:** Automated responses can be augmented with current and relevant company data or user manuals.\n",
      "\n",
      "- **Content Generation:** Generating informative articles or summaries where referencing actual documents enhances content credibility and detail.\n",
      "\n",
      "The RAG pipeline is a powerful tool that leverages the strengths of both retrieval and generation, achieving superior results compared to using either approach independently.\n",
      "QuizMaster: Here are a few practice questions based on the explanation of the RAG Pipeline:\n",
      "\n",
      "1. **Describe the role of the retrieval component in the RAG pipeline. What methods can be employed for retrieval, and what is the expected output of this step?**\n",
      "\n",
      "2. **Explain how the augmentation step enhances the process of text generation within the RAG pipeline. What specific information does it use to enrich the generative model's context?**\n",
      "\n",
      "3. **How does the RAG pipeline improve the accuracy and contextuality of responses compared to using a standalone generative model? Provide at least two reasons based on the explanation.**\n",
      "\n",
      "4. **List and describe two specific use cases where the RAG pipeline could be particularly beneficial. How does this approach improve the output quality in these applications?**\n",
      "\n",
      "5. **In the context of the RAG pipeline, what are the advantages of splitting retrieval from generation? Discuss how this separation enhances the system's capability and scalability.**\n",
      "Grader: 1. **Retrieval Role:** Finds relevant documents from a large corpus to inform response generation. Methods include TF-IDF, BM25, or neural models like DPR. Output: Relevant documents/pieces for the query.\n",
      "   \n",
      "2. **Augmentation Benefit:** Enriches the generative model by providing context from retrieved documents, ensuring responses are more informed.\n",
      "\n",
      "3. **Improved Accuracy:** Combines retrieved real-world data with the generative model; handles specific queries effectively with up-to-date info.\n",
      "\n",
      "4. **Use Cases:** \n",
      "   - **Question Answering:** Produces informed responses by referencing real data.\n",
      "   - **Customer Support:** Enhances automated responses with current information.\n",
      "\n",
      "5. **Advantages of Separation:** Enables handling of large datasets efficiently and improves system scalability by decoupling processes.\n",
      "Motivator: Fantastic work! You've clearly grasped the key elements of the RAG pipeline and its advantages over standalone models. Keep practicing, and you'll be able to teach others in no time. TERMINATE.\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.messages import TextMessage\n",
    "import asyncio\n",
    "\n",
    "async def run_team():\n",
    "    task = TextMessage(\n",
    "        content=\"Explain about RAG Pipeline.\",\n",
    "        source=\"User\"\n",
    "    )\n",
    "\n",
    "    result = await edu_team.run(task=task)\n",
    "\n",
    "    for msg in result.messages:\n",
    "        print(f\"{msg.source}: {msg.content}\")\n",
    "\n",
    "await run_team()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
